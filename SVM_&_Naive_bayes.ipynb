{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNolOrJKeIc0",
        "outputId": "cc55ca90-7d51-4d0c-d607-9e058cfef701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the SVM classifier: 1.00\n"
          ]
        }
      ],
      "source": [
        "# 1. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "# You can experiment with different kernels (e.g., 'linear', 'poly', 'rbf')\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of the SVM classifier: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n",
        "# compare their accuracies\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train an SVM classifier with a linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for the linear kernel\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "\n",
        "# Create and train an SVM classifier with an RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate accuracy for the RBF kernel\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "# Print the accuracies for comparison\n",
        "print(f\"Accuracy of SVM with Linear Kernel: {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy of SVM with RBF Kernel: {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw5V7cZLeJRS",
        "outputId": "920386e3-018b-4902-ea4f-28342a8bc47e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear Kernel: 0.98\n",
            "Accuracy of SVM with RBF Kernel: 0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.  Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n",
        "# Squared Error (MSE)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load a housing dataset (using California Housing dataset as an example)\n",
        "# If you have your own dataset, load it here\n",
        "try:\n",
        "    housing = datasets.fetch_california_housing()\n",
        "    X = housing.data\n",
        "    y = housing.target\n",
        "except AttributeError:\n",
        "    print(\"California Housing dataset not available. Using make_regression as an example.\")\n",
        "    from sklearn.datasets import make_regression\n",
        "    X, y = make_regression(n_samples=200, n_features=10, noise=10, random_state=42)\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVR regressor\n",
        "# You can experiment with different kernels (e.g., 'linear', 'poly', 'rbf') and parameters (e.g., C, epsilon)\n",
        "svr_regressor = SVR(kernel='linear') # You can change the kernel here\n",
        "\n",
        "# Train the regressor\n",
        "svr_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the regressor using Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "# Print the Mean Squared Error\n",
        "print(f\"Mean Squared Error (MSE) of the SVR regressor: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "FgNx_kl8ew7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4.  Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n",
        "# boundary\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset (for simplicity in visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # We only take the first two features for easy visualization\n",
        "y = iris.target\n",
        "\n",
        "# Filter to only use two classes for simpler visualization\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier with a polynomial kernel\n",
        "# degree: the degree of the polynomial kernel\n",
        "# C: regularization parameter\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Visualize the decision boundary\n",
        "# Create a meshgrid to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Predict the class for each point in the meshgrid\n",
        "Z = svm_poly.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "# Plot the decision boundary\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "# Plot the training points\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "plt.xlabel('Sepal length')\n",
        "plt.ylabel('Sepal width')\n",
        "plt.title('SVM with Polynomial Kernel Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_6Z91o5eJTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n",
        "# evaluate accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Accuracy of the Gaussian Naive Bayes classifier: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "XGrXt3lGeJWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n",
        "# Newsgroups dataset\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "# We can select a subset of categories for faster execution, or load all of them.\n",
        "# Here, we'll load a few related categories.\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Convert text data into numerical feature vectors using TF-IDF\n",
        "# TF-IDF (Term Frequency-Inverse Document Frequency) is a common text vectorization technique.\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Create a Multinomial Naive Bayes classifier\n",
        "# Multinomial Naive Bayes is well-suited for text data where features are counts or frequencies.\n",
        "multinomial_nb = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "multinomial_nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = multinomial_nb.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Multinomial Naive Bayes classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_test.target_names))"
      ],
      "metadata": {
        "id": "XJuVNXW3eJYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to train an SVM Classifier with different C values and compare the decision\n",
        "# boundaries visually\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Load the Iris dataset (for simplicity in visualization)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[:, :2]  # We only take the first two features for easy visualization\n",
        "y = iris.target\n",
        "\n",
        "# Filter to only use two classes for simpler visualization\n",
        "X = X[y != 2]\n",
        "y = y[y != 2]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define different C values to experiment with\n",
        "c_values = [0.1, 1, 10, 100]\n",
        "\n",
        "# Create a meshgrid to plot the decision boundary\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "\n",
        "# Iterate through different C values and visualize the decision boundaries\n",
        "for c in c_values:\n",
        "    # Create an SVM classifier with the current C value and RBF kernel\n",
        "    # We use RBF kernel here as it often shows more interesting decision boundaries\n",
        "    svm_classifier = SVC(kernel='rbf', C=c, random_state=42)\n",
        "\n",
        "    # Train the classifier\n",
        "    svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predict the class for each point in the meshgrid\n",
        "    Z = svm_classifier.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.figure()\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
        "\n",
        "    # Plot the training points\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolors='k')\n",
        "    plt.xlabel('Sepal length')\n",
        "    plt.ylabel('Sepal width')\n",
        "    plt.title(f'SVM with RBF Kernel (C={c}) Decision Boundary')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "DIlb0wsieJaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8.  Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n",
        "# binary features\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a synthetic dataset with binary features\n",
        "# In a real-world scenario, you would load your binary dataset here.\n",
        "# For demonstration, we'll create a simple dataset.\n",
        "# Let's imagine features represent whether a customer bought certain products (1) or not (0).\n",
        "# The target variable is whether they made a purchase on a specific day (1) or not (0).\n",
        "\n",
        "# Number of samples\n",
        "n_samples = 1000\n",
        "# Number of binary features\n",
        "n_features = 10\n",
        "\n",
        "# Generate random binary features (0 or 1)\n",
        "X = np.random.randint(0, 2, size=(n_samples, n_features))\n",
        "\n",
        "# Create a synthetic binary target variable based on some simple rule\n",
        "# For example, if features 0 and 3 are 1, the target is more likely to be 1\n",
        "y = np.zeros(n_samples)\n",
        "y[(X[:, 0] == 1) & (X[:, 3] == 1)] = 1\n",
        "y[np.random.rand(n_samples) < 0.1] = 1 # Add some noise/random positive cases\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Bernoulli Naive Bayes classifier\n",
        "# BernoulliNB is suitable for binary features. It assumes features are independent boolean variables. [1] [2]\n",
        "bnb = BernoulliNB()\n",
        "\n",
        "# Train the classifier\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = bnb.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Bernoulli Naive Bayes classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "_j0siBIAeJdH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9.  Write a Python program to apply feature scaling before training an SVM model and compare results with\n",
        "# unscaled data\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load a dataset (using Iris dataset as an example)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train SVM on unscaled data ---\n",
        "\n",
        "# Create an SVM classifier (using RBF kernel, which is sensitive to feature scaling)\n",
        "svm_unscaled = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the classifier on unscaled data\n",
        "svm_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the unscaled test set\n",
        "y_pred_unscaled = svm_unscaled.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy on unscaled data\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy of SVM on unscaled data: {accuracy_unscaled:.2f}\")\n",
        "\n",
        "# --- Apply Feature Scaling ---\n",
        "\n",
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform it\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the same scaler\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Train SVM on scaled data ---\n",
        "\n",
        "# Create an SVM classifier with the same parameters\n",
        "svm_scaled = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# Train the classifier on scaled data\n",
        "svm_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled test set\n",
        "y_pred_scaled = svm_scaled.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate accuracy on scaled data\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy of SVM on scaled data: {accuracy_scaled:.2f}\")"
      ],
      "metadata": {
        "id": "KmplPYvYeJfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10.  Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n",
        "#  Laplace Smoothing\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a synthetic dataset with a feature that might have zero variance in a class\n",
        "# For demonstration, let's make a feature constant for one class\n",
        "X = np.array([[1.0, 2.0, 3.0],\n",
        "              [1.1, 2.1, 3.2],\n",
        "              [1.2, 2.2, 3.1],\n",
        "              [5.0, 6.0, 7.0],\n",
        "              [5.1, 6.2, 7.1],\n",
        "              [5.3, 6.1, 7.3]])\n",
        "\n",
        "y = np.array([0, 0, 0, 1, 1, 1]) # Two classes\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
        "\n",
        "print(\"Training data:\\n\", X_train)\n",
        "print(\"Training labels:\", y_train)\n",
        "print(\"Test data:\\n\", X_test)\n",
        "print(\"Test labels:\", y_test)\n",
        "\n",
        "# --- Train Gaussian Naive Bayes without explicit smoothing (default) ---\n",
        "# scikit-learn's GaussianNB has a 'var_smoothing' parameter\n",
        "\n",
        "print(\"\\n--- Gaussian Naive Bayes without explicit variance smoothing (default) ---\")\n",
        "gnb_default = GaussianNB()\n",
        "gnb_default.fit(X_train, y_train)\n",
        "\n",
        "# Print estimated parameters (mean and variance) for each class\n",
        "print(\"Mean (feature 0, Class 0):\", gnb_default.theta_[0, 0])\n",
        "print(\"Variance (feature 0, Class 0):\", gnb_default.sigma_[0, 0])\n",
        "print(\"Mean (feature 0, Class 1):\", gnb_default.theta_[1, 0])\n",
        "print(\"Variance (feature 0, Class 1):\", gnb_default.sigma_[1, 0])\n",
        "\n",
        "\n",
        "y_pred_default = gnb_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(f\"Accuracy (default smoothing): {accuracy_default:.2f}\")\n",
        "print(\"Classification Report (default smoothing):\\n\", classification_report(y_test, y_pred_default))\n",
        "\n",
        "\n",
        "# --- Train Gaussian Naive Bayes with variance smoothing ---\n",
        "# Use the 'var_smoothing' parameter in GaussianNB\n",
        "\n",
        "print(\"\\n--- Gaussian Naive Bayes with variance smoothing (e.g., var_smoothing=1e-9) ---\")\n",
        "gnb_smoothed = GaussianNB(var_smoothing=1e-9) # Add a small value to the variance\n",
        "gnb_smoothed.fit(X_train, y_train)\n",
        "\n",
        "# Print estimated parameters with smoothing\n",
        "print(\"Mean (feature 0, Class 0):\", gnb_smoothed.theta_[0, 0])\n",
        "print(\"Variance (feature 0, Class 0) with smoothing:\", gnb_smoothed.sigma_[0, 0])\n",
        "print(\"Mean (feature 0, Class 1):\", gnb_smoothed.theta_[1, 0])\n",
        "print(\"Variance (feature 0, Class 1) with smoothing:\", gnb_smoothed.sigma_[1, 0])\n",
        "\n",
        "\n",
        "y_pred_smoothed = gnb_smoothed.predict(X_test)\n",
        "accuracy_smoothed = accuracy_score(y_test, y_pred_smoothed)\n",
        "print(f\"Accuracy (with variance smoothing): {accuracy_smoothed:.2f}\")\n",
        "print(\"Classification Report (with variance smoothing):\\n\", classification_report(y_test, y_pred_smoothed))\n",
        "\n",
        "# Compare predictions\n",
        "print(\"\\nComparison of predictions:\")\n",
        "print(\"Test labels:\", y_test)\n",
        "print(\"Predictions (default):\", y_pred_default)\n",
        "print(\"Predictions (smoothed):\", y_pred_smoothed)"
      ],
      "metadata": {
        "id": "lKYCBAYFeJht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11.  Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n",
        "# gamma, kernel)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the parameter grid for GridSearchCV\n",
        "# This is a dictionary where keys are hyperparameters and values are lists of values to try.\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],          # Regularization parameter\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],  # Kernel coefficient for 'rbf', 'poly', 'sigmoid'\n",
        "    'kernel': ['rbf', 'linear']      # Type of kernel\n",
        "}\n",
        "\n",
        "# Create an SVM classifier instance\n",
        "svm = SVC()\n",
        "\n",
        "# Create a GridSearchCV object\n",
        "# estimator: The model to tune (SVC in this case)\n",
        "# param_grid: The grid of hyperparameters to search\n",
        "# cv: Number of folds for cross-validation\n",
        "# scoring: The metric to evaluate the model (e.g., 'accuracy')\n",
        "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "# This will perform the grid search with cross-validation\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters found by GridSearchCV\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Print the best cross-validation score\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Get the best model\n",
        "best_svm_model = grid_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "y_pred = best_svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy on the test set\n",
        "print(f\"Accuracy of the best SVM model on the test set: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "StutXC3yeJj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12.  Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n",
        "# check it improve accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a synthetic imbalanced dataset\n",
        "# Let's create a dataset with two classes, where one class is significantly larger than the other.\n",
        "n_samples_total = 1000\n",
        "n_samples_majority = int(n_samples_total * 0.9) # 90% majority class\n",
        "n_samples_minority = n_samples_total - n_samples_majority # 10% minority class\n",
        "\n",
        "# Generate synthetic features (example: normally distributed features)\n",
        "X_majority = np.random.rand(n_samples_majority, 2) * 5\n",
        "X_minority = np.random.rand(n_samples_minority, 2) * 2 + 3 # Shift minority to overlap\n",
        "\n",
        "# Create target labels\n",
        "y_majority = np.zeros(n_samples_majority)\n",
        "y_minority = np.ones(n_samples_minority)\n",
        "\n",
        "# Combine majority and minority classes\n",
        "X = np.vstack((X_majority, X_minority))\n",
        "y = np.hstack((y_majority, y_minority))\n",
        "\n",
        "# Shuffle the dataset\n",
        "indices = np.arange(n_samples_total)\n",
        "np.random.shuffle(indices)\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of samples in majority class (0): {np.sum(y == 0)}\")\n",
        "print(f\"Number of samples in minority class (1): {np.sum(y == 1)}\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "# Using stratify=y is important for imbalanced datasets to maintain the class distribution in train/test splits\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Number of samples in training majority class (0): {np.sum(y_train == 0)}\")\n",
        "print(f\"Number of samples in training minority class (1): {np.sum(y_train == 1)}\")\n",
        "\n",
        "# --- Train SVM without class weighting ---\n",
        "\n",
        "print(\"\\n--- Training SVM without class weighting ---\")\n",
        "svm_no_weight = SVC(kernel='linear', random_state=42)\n",
        "svm_no_weight.fit(X_train, y_train)\n",
        "y_pred_no_weight = svm_no_weight.predict(X_test)\n",
        "\n",
        "accuracy_no_weight = accuracy_score(y_test, y_pred_no_weight)\n",
        "print(f\"Accuracy (no weighting): {accuracy_no_weight:.2f}\")\n",
        "print(\"Classification Report (no weighting):\\n\", classification_report(y_test, y_pred_no_weight))\n",
        "\n",
        "# --- Train SVM with class weighting ---\n",
        "\n",
        "print(\"\\n--- Training SVM with class weighting ---\")\n",
        "# The 'balanced' mode automatically adjusts weights inversely proportional to class frequencies\n",
        "svm_with_weight = SVC(kernel='linear', class_weight='balanced', random_state=42)\n",
        "svm_with_weight.fit(X_train, y_train)\n",
        "y_pred_with_weight = svm_with_weight.predict(X_test)\n",
        "\n",
        "accuracy_with_weight = accuracy_score(y_test, y_pred_with_weight)\n",
        "print(f\"Accuracy (with weighting): {accuracy_with_weight:.2f}\")\n",
        "print(\"Classification Report (with weighting):\\n\", classification_report(y_test, y_pred_with_weight))\n",
        "\n",
        "# You can visually compare the classification reports to see the effect of class weighting on\n",
        "# precision, recall, and F1-score for the minority class."
      ],
      "metadata": {
        "id": "MKVXHHe9eJmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer # or TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a small synthetic dataset of emails and their labels (0 for ham, 1 for spam)\n",
        "emails = [\n",
        "    'Hey, how are you doing?',\n",
        "    'Buy cheap viagra now',\n",
        "    'Meet me for lunch tomorrow',\n",
        "    'Get your free prize!',\n",
        "    'Just checking in',\n",
        "    'Claim your reward!',\n",
        "    'Hi, let\\'s catch up soon',\n",
        "    'Spam email with lots of keywords like free money now',\n",
        "    'Project update meeting at 2 PM',\n",
        "    'Earn money online fast'\n",
        "]\n",
        "\n",
        "labels = [0, 1, 0, 1, 0, 1, 0, 1, 0, 1] # 0 for ham, 1 for spam\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails, labels, test_size=0.3, random_state=42)\n",
        "\n",
        "# Convert text data into numerical feature vectors using CountVectorizer\n",
        "# CountVectorizer converts a collection of text documents to a matrix of token counts.\n",
        "# Alternatively, TfidfVectorizer can be used for TF-IDF features.\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectors = vectorizer.fit_transform(X_train)\n",
        "X_test_vectors = vectorizer.transform(X_test)\n",
        "\n",
        "# Create a Multinomial Naive Bayes classifier\n",
        "# MultinomialNB is suitable for discrete counts, like word counts in text.\n",
        "multinomial_nb = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "multinomial_nb.fit(X_train_vectors, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = multinomial_nb.predict(X_test_vectors)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Naive Bayes spam classifier: {accuracy:.2f}\")\n",
        "\n",
        "# Print a detailed classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))\n",
        "\n",
        "# Example of classifying a new email\n",
        "new_email = [\"Claim your free gift card now!\"]\n",
        "new_email_vector = vectorizer.transform(new_email)\n",
        "prediction = multinomial_nb.predict(new_email_vector)\n",
        "\n",
        "print(f\"\\nNew email: '{new_email[0]}'\")\n",
        "print(f\"Prediction: {'Spam' if prediction[0] == 1 else 'Ham'}\")\n",
        "\n",
        "new_email_2 = [\"Hey, just wanted to follow up on the meeting.\"]\n",
        "new_email_vector_2 = vectorizer.transform(new_email_2)\n",
        "prediction_2 = multinomial_nb.predict(new_email_vector_2)\n",
        "\n",
        "print(f\"\\nNew email: '{new_email_2[0]}'\")\n",
        "print(f\"Prediction: {'Spam' if prediction_2[0] == 1 else 'Ham'}\")\n"
      ],
      "metadata": {
        "id": "uHpSnIWgeJo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14.  Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n",
        "# compare their accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB  # Using GaussianNB as an example\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load a dataset (using the Iris dataset as an example)\n",
        "data = datasets.load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train SVM Classifier ---\n",
        "\n",
        "print(\"--- Training SVM Classifier ---\")\n",
        "# Create an SVM classifier (you can choose different kernels)\n",
        "svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the SVM classifier\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"Accuracy of the SVM classifier: {accuracy_svm:.2f}\")\n",
        "print(\"Classification Report (SVM):\\n\", classification_report(y_test, y_pred_svm, target_names=data.target_names))\n",
        "\n",
        "# --- Train Naive Bayes Classifier ---\n",
        "\n",
        "print(\"\\n--- Training Naive Bayes Classifier ---\")\n",
        "# Create a Naive Bayes classifier (using GaussianNB for this dataset)\n",
        "# For text data, you might use MultinomialNB\n",
        "naive_bayes_classifier = GaussianNB()\n",
        "\n",
        "# Train the classifier\n",
        "naive_bayes_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_nb = naive_bayes_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy of the Naive Bayes classifier\n",
        "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "print(f\"Accuracy of the Naive Bayes classifier: {accuracy_nb:.2f}\")\n",
        "print(\"Classification Report (Naive Bayes):\\n\", classification_report(y_test, y_pred_nb, target_names=data.target_names))\n",
        "\n",
        "# --- Compare Accuracies ---\n",
        "\n",
        "print(\"\\n--- Comparing Accuracies ---\")\n",
        "print(f\"SVM Classifier Accuracy: {accuracy_svm:.2f}\")\n",
        "print(f\"Naive Bayes Classifier Accuracy: {accuracy_nb:.2f}\")\n"
      ],
      "metadata": {
        "id": "pN8-zIvAeJrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 15.  Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n",
        "# results\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_selection import SelectKBest, chi2  # Import SelectKBest and chi2\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Convert text data into numerical feature vectors using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Shape of TF-IDF vectors before feature selection: {X_train_tfidf.shape}\")\n",
        "\n",
        "# --- Train Multinomial Naive Bayes without Feature Selection ---\n",
        "\n",
        "print(\"\\n--- Training Multinomial Naive Bayes without Feature Selection ---\")\n",
        "multinomial_nb_no_fs = MultinomialNB()\n",
        "multinomial_nb_no_fs.fit(X_train_tfidf, y_train)\n",
        "y_pred_no_fs = multinomial_nb_no_fs.predict(X_test_tfidf)\n",
        "accuracy_no_fs = accuracy_score(y_test, y_pred_no_fs)\n",
        "print(f\"Accuracy without Feature Selection: {accuracy_no_fs:.2f}\")\n",
        "print(\"Classification Report (without Feature Selection):\\n\", classification_report(y_test, y_pred_no_fs, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# --- Apply Feature Selection ---\n",
        "\n",
        "# Use SelectKBest to select the top K features based on chi-squared statistics\n",
        "# chi2 works well with non-negative data like TF-IDF.\n",
        "# You need to choose an appropriate value for k (number of features to select).\n",
        "k_best_features = 1000 # Example: select the top 1000 features\n",
        "\n",
        "selector = SelectKBest(chi2, k=k_best_features)\n",
        "\n",
        "# Fit the selector on the training data and transform both training and test data\n",
        "X_train_selected = selector.fit_transform(X_train_tfidf, y_train)\n",
        "X_test_selected = selector.transform(X_test_tfidf)\n",
        "\n",
        "print(f\"\\nShape of TF-IDF vectors after feature selection: {X_train_selected.shape}\")\n",
        "\n",
        "# --- Train Multinomial Naive Bayes with Feature Selection ---\n",
        "\n",
        "print(\"\\n--- Training Multinomial Naive Bayes with Feature Selection ---\")\n",
        "multinomial_nb_with_fs = MultinomialNB()\n",
        "multinomial_nb_with_fs.fit(X_train_selected, y_train)\n",
        "y_pred_with_fs = multinomial_nb_with_fs.predict(X_test_selected)\n",
        "accuracy_with_fs = accuracy_score(y_test, y_pred_with_fs)\n",
        "print(f\"Accuracy with Feature Selection: {accuracy_with_fs:.2f}\")\n",
        "print(\"Classification Report (with Feature Selection):\\n\", classification_report(y_test, y_pred_with_fs, target_names=newsgroups_test.target_names))\n",
        "\n",
        "# --- Compare Accuracies ---\n",
        "\n",
        "print(\"\\n--- Comparing Accuracies ---\")\n",
        "print(f\"Accuracy without Feature Selection: {accuracy_no_fs:.2f}\")\n",
        "print(f\"Accuracy with Feature Selection: {accuracy_with_fs:.2f}\")"
      ],
      "metadata": {
        "id": "8iONnbgkeJww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n",
        "# strategies on the Wine dataset and compare their accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Wine dataset (which is a multiclass dataset)\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train SVM using One-vs-Rest (OvR) strategy ---\n",
        "\n",
        "print(\"--- Training SVM using One-vs-Rest (OvR) strategy ---\")\n",
        "# Create a base binary SVM classifier\n",
        "base_svm_ovr = SVC(kernel='linear', random_state=42) # You can use different kernels\n",
        "\n",
        "# Create the OneVsRestClassifier wrapper with the base SVM\n",
        "ovr_classifier = OneVsRestClassifier(base_svm_ovr)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovr = ovr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy and report\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of SVM with OvR strategy: {accuracy_ovr:.2f}\")\n",
        "print(\"Classification Report (OvR):\\n\", classification_report(y_test, y_pred_ovr, target_names=wine.target_names))\n",
        "\n",
        "# --- Train SVM using One-vs-One (OvO) strategy ---\n",
        "\n",
        "print(\"\\n--- Training SVM using One-vs-One (OvO) strategy ---\")\n",
        "# Create a base binary SVM classifier (can be the same as for OvR)\n",
        "base_svm_ovo = SVC(kernel='linear', random_state=42) # Using the same base SVM\n",
        "\n",
        "# Create the OneVsOneClassifier wrapper with the base SVM\n",
        "ovo_classifier = OneVsOneClassifier(base_svm_ovo)\n",
        "\n",
        "# Train the OvO classifier\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_ovo = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy and report\n",
        "accuracy_ovo = accuracy_score(y_test, y_pred_ovo)\n",
        "print(f\"Accuracy of SVM with OvO strategy: {accuracy_ovo:.2f}\")\n",
        "print(\"Classification Report (OvO):\\n\", classification_report(y_test, y_pred_ovo, target_names=wine.target_names))\n",
        "\n",
        "# --- Compare Accuracies ---\n",
        "\n",
        "print(\"\\n--- Comparing Accuracies ---\")\n",
        "print(f\"SVM with OvR Accuracy: {accuracy_ovr:.2f}\")\n",
        "print(f\"SVM with OvO Accuracy: {accuracy_ovo:.2f}\")"
      ],
      "metadata": {
        "id": "efhfQ-DShgCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n",
        "# Cancer dataset and compare their accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# --- Train SVM with Linear Kernel ---\n",
        "\n",
        "print(\"--- Training SVM with Linear Kernel ---\")\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "print(f\"Accuracy (Linear Kernel): {accuracy_linear:.2f}\")\n",
        "print(\"Classification Report (Linear Kernel):\\n\", classification_report(y_test, y_pred_linear, target_names=breast_cancer.target_names))\n",
        "\n",
        "# --- Train SVM with Polynomial Kernel ---\n",
        "\n",
        "print(\"\\n--- Training SVM with Polynomial Kernel ---\")\n",
        "# You can adjust the 'degree' and 'C' parameters for the polynomial kernel\n",
        "svm_poly = SVC(kernel='poly', degree=3, C=1.0, random_state=42)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "y_pred_poly = svm_poly.predict(X_test)\n",
        "accuracy_poly = accuracy_score(y_test, y_pred_poly)\n",
        "print(f\"Accuracy (Polynomial Kernel): {accuracy_poly:.2f}\")\n",
        "print(\"Classification Report (Polynomial Kernel):\\n\", classification_report(y_test, y_pred_poly, target_names=breast_cancer.target_names))\n",
        "\n",
        "# --- Train SVM with RBF Kernel ---\n",
        "\n",
        "print(\"\\n--- Training SVM with RBF Kernel ---\")\n",
        "# You can adjust the 'gamma' and 'C' parameters for the RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', gamma='scale', C=1.0, random_state=42) # 'scale' is a good default for gamma\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "print(f\"Accuracy (RBF Kernel): {accuracy_rbf:.2f}\")\n",
        "print(\"Classification Report (RBF Kernel):\\n\", classification_report(y_test, y_pred_rbf, target_names=breast_cancer.target_names))\n",
        "\n",
        "# --- Compare Accuracies ---\n",
        "\n",
        "print(\"\\n--- Comparing Accuracies ---\")\n",
        "print(f\"Accuracy (Linear Kernel): {accuracy_linear:.2f}\")\n",
        "print(f\"Accuracy (Polynomial Kernel): {accuracy_poly:.2f}\")\n",
        "print(f\"Accuracy (RBF Kernel): {accuracy_rbf:.2f}\")"
      ],
      "metadata": {
        "id": "MuOy-pOvhf--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n",
        "# average accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42) # You can choose other kernels\n",
        "\n",
        "# Create a Stratified K-Fold cross-validation object\n",
        "# n_splits: The number of folds (e.g., 5 or 10 are common)\n",
        "# shuffle: Whether to shuffle the data before splitting (usually recommended)\n",
        "# random_state: For reproducibility when shuffling\n",
        "n_splits = 5\n",
        "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation using cross_val_score\n",
        "# estimator: The model to evaluate (SVM classifier)\n",
        "# X, y: The data and target variable\n",
        "# cv: The cross-validation splitting strategy (our StratifiedKFold object)\n",
        "# scoring: The evaluation metric (e.g., 'accuracy')\n",
        "scores = cross_val_score(svm_classifier, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Print the scores for each fold\n",
        "print(f\"Accuracy scores for each fold: {scores}\")\n",
        "\n",
        "# Compute and print the average accuracy\n",
        "average_accuracy = np.mean(scores)\n",
        "print(f\"Average accuracy: {average_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "bolnCXvthf6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n",
        "#nperformance\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Create a synthetic imbalanced dataset with simple features\n",
        "# Class 0 (Majority), Class 1 (Minority)\n",
        "X = np.array([\n",
        "    [1, 0, 0], # Class 0\n",
        "    [1, 1, 0], # Class 0\n",
        "    [1, 0, 1], # Class 0\n",
        "    [1, 0, 0], # Class 0\n",
        "    [1, 1, 0], # Class 0\n",
        "    [0, 1, 1], # Class 1\n",
        "    [0, 1, 1]  # Class 1\n",
        "])\n",
        "y = np.array([0, 0, 0, 0, 0, 1, 1])\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Class distribution (0: Majority, 1: Minority): {np.bincount(y)}\")\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "# Using stratify=y is recommended for imbalanced datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"\\nTraining set class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test set class distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "# --- Train Multinomial Naive Bayes with default priors ---\n",
        "\n",
        "print(\"\\n--- Training Multinomial Naive Bayes with Default Priors ---\")\n",
        "multinomial_nb_default = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "multinomial_nb_default.fit(X_train, y_train)\n",
        "\n",
        "# Print learned priors\n",
        "print(\"Learned priors (default):\", np.exp(multinomial_nb_default.class_log_prior_))\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_default = multinomial_nb_default.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(f\"Accuracy (default priors): {accuracy_default:.2f}\")\n",
        "print(\"Classification Report (default priors):\\n\", classification_report(y_test, y_pred_default))\n",
        "\n",
        "# --- Train Multinomial Naive Bayes with custom priors ---\n",
        "\n",
        "print(\"\\n--- Training Multinomial Naive Bayes with Custom Priors ---\")\n",
        "# Define custom prior probabilities\n",
        "# These should sum to 1 and be in the order of your class labels (e.g., [prior_class_0, prior_class_1])\n",
        "custom_priors = np.array([0.3, 0.7]) # Example: assigning higher prior to the minority class\n",
        "\n",
        "# Create a Multinomial Naive Bayes classifier with custom priors\n",
        "multinomial_nb_custom = MultinomialNB(priors=custom_priors)\n",
        "\n",
        "# Train the classifier\n",
        "# When priors are specified, the fit method does not learn the priors from the data.\n",
        "multinomial_nb_custom.fit(X_train, y_train)\n",
        "\n",
        "# Print the priors used\n",
        "print(\"Used priors (custom):\", np.exp(multinomial_nb_custom.class_log_prior_))\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_custom = multinomial_nb_custom.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy_custom = accuracy_score(y_test, y_pred_custom)\n",
        "print(f\"Accuracy (custom priors): {accuracy_custom:.2f}\")\n",
        "print(\"Classification Report (custom priors):\\n\", classification_report(y_test, y_pred_custom))\n",
        "\n",
        "# --- Compare Predictions ---\n",
        "print(\"\\nComparison of predictions:\")\n",
        "print(\"Test labels:\", y_test)\n",
        "print(\"Predictions (default priors):\", y_pred_default)\n",
        "print(\"Predictions (custom priors):\", y_pred_custom)"
      ],
      "metadata": {
        "id": "gGCrY2Ulhf3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n",
        "# compare accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "print(f\"Shape of data before RFE: {X_train.shape}\")\n",
        "\n",
        "# --- Train SVM on all features ---\n",
        "\n",
        "print(\"\\n--- Training SVM on all features ---\")\n",
        "svm_all_features = SVC(kernel='linear', random_state=42)\n",
        "svm_all_features.fit(X_train, y_train)\n",
        "y_pred_all = svm_all_features.predict(X_test)\n",
        "accuracy_all = accuracy_score(y_test, y_pred_all)\n",
        "print(f\"Accuracy with all features: {accuracy_all:.2f}\")\n",
        "print(\"Classification Report (all features):\\n\", classification_report(y_test, y_pred_all, target_names=iris.target_names))\n",
        "\n",
        "# --- Perform Recursive Feature Elimination (RFE) ---\n",
        "\n",
        "# Create a base SVM classifier (must support feature importance or coefficients)\n",
        "# A linear kernel SVM (SVC or LinearSVC) or a linear model works well with RFE\n",
        "estimator = SVC(kernel='linear') # Using a linear SVM as the base estimator\n",
        "\n",
        "# Create the RFE object\n",
        "# estimator: The base model to use for feature ranking\n",
        "# n_features_to_select: The desired number of features to keep\n",
        "rfe = RFE(estimator=estimator, n_features_to_select=2) # Example: Select the top 2 features\n",
        "\n",
        "# Fit RFE on the training data\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Print the ranking of features (1 being the most important)\n",
        "print(\"\\nFeature ranking by RFE:\")\n",
        "# Create a list of tuples (feature_name, rank)\n",
        "ranked_features = sorted([(feature_names[i], rank) for i, rank in enumerate(rfe.ranking_)])\n",
        "for name, rank in ranked_features:\n",
        "    print(f\"  {name}: Rank {rank}\")\n",
        "\n",
        "# Print the selected features (True means selected, False means eliminated)\n",
        "print(\"\\nSelected features by RFE:\")\n",
        "print(rfe.support_)\n",
        "selected_feature_indices = np.where(rfe.support_)[0]\n",
        "print(\"Selected feature indices:\", selected_feature_indices)\n",
        "print(\"Selected feature names:\", [feature_names[i] for i in selected_feature_indices])\n",
        "\n",
        "# Transform the training and test data to include only the selected features\n",
        "X_train_rfe = rfe.transform(X_train)\n",
        "X_test_rfe = rfe.transform(X_test)\n",
        "\n",
        "print(f\"\\nShape of data after RFE: {X_train_rfe.shape}\")\n",
        "\n",
        "# --- Train SVM on selected features ---\n",
        "\n",
        "print(\"\\n--- Training SVM on selected features ---\")\n",
        "svm_rfe_features = SVC(kernel='linear', random_state=42) # Use the same kernel as before\n",
        "svm_rfe_features.fit(X_train_rfe, y_train)\n",
        "y_pred_rfe = svm_rfe_features.predict(X_test_rfe)\n",
        "accuracy_rfe = accuracy_score(y_test, y_pred_rfe)\n",
        "print(f\"Accuracy with RFE selected features: {accuracy_rfe:.2f}\")\n",
        "print(\"Classification Report (RFE selected features):\\n\", classification_report(y_test, y_pred_rfe, target_names=iris.target_names))\n",
        "\n",
        "# --- Compare Accuracies ---\n",
        "\n",
        "print(\"\\n--- Comparing Accuracies ---\")\n",
        "print(f\"Accuracy with all features: {accuracy_all:.2f}\")\n",
        "print(f\"Accuracy with RFE selected features: {accuracy_rfe:.2f}\")\n"
      ],
      "metadata": {
        "id": "-EdNigYZhf1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n",
        "# F1-Score instead of accuracy\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target # 0 for malignant, 1 for benign\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42) # Using linear kernel as an example\n",
        "\n",
        "# Train the classifier\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# --- Evaluate using Precision, Recall, and F1-Score ---\n",
        "\n",
        "# Calculate Precision\n",
        "# Precision is the ratio of correctly predicted positive observations to the total predicted positive observations.\n",
        "# A high precision relates to a low false positive rate.\n",
        "# For binary classification, you need to specify the positive label if it's not the default (1).\n",
        "precision = precision_score(y_test, y_pred, pos_label=1) # Assuming 1 is the positive class (benign)\n",
        "\n",
        "# Calculate Recall (Sensitivity)\n",
        "# Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes.\n",
        "# A high recall relates to a low false negative rate.\n",
        "recall = recall_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "# Calculate F1-Score\n",
        "# The F1-score is the harmonic mean of Precision and Recall. [1]\n",
        "# It is a weighted average of the precision and recall, where an F1-score reaches its best value at 1 and worst at 0.\n",
        "f1 = f1_score(y_test, y_pred, pos_label=1)\n",
        "\n",
        "# Print the calculated metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# --- Alternatively, use classification_report for all metrics ---\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "# The classification_report function provides precision, recall, F1-score, and support for each class.\n",
        "print(classification_report(y_test, y_pred, target_names=breast_cancer.target_names))"
      ],
      "metadata": {
        "id": "-wLmnAqZhfxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 22.  Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n",
        "# (Cross-Entropy Loss)\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import log_loss, accuracy_score # Import log_loss\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "# We can select a subset of categories for faster execution\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "X_train = newsgroups_train.data\n",
        "y_train = newsgroups_train.target\n",
        "X_test = newsgroups_test.data\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "# Convert text data into numerical feature vectors using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Create a Multinomial Naive Bayes classifier\n",
        "multinomial_nb = MultinomialNB()\n",
        "\n",
        "# Train the classifier\n",
        "multinomial_nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# --- Evaluate using Log Loss ---\n",
        "\n",
        "# Get the predicted probabilities for the test set\n",
        "# log_loss requires probability estimates (output of predict_proba)\n",
        "y_pred_proba = multinomial_nb.predict_proba(X_test_tfidf)\n",
        "\n",
        "# Calculate Log Loss\n",
        "# The first argument is the true labels (y_test)\n",
        "# The second argument is the predicted probabilities (y_pred_proba)\n",
        "logloss = log_loss(y_test, y_pred_proba)\n",
        "\n",
        "# Print the Log Loss\n",
        "print(f\"Log Loss (Cross-Entropy) of the Naive Bayes classifier: {logloss:.4f}\")\n",
        "\n",
        "# --- Also print accuracy for comparison ---\n",
        "# (Accuracy is calculated from the predicted class, not probabilities)\n",
        "y_pred = multinomial_nb.predict(X_test_tfidf)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Naive Bayes classifier: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "KVsvyHLThfup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn\n",
        "\n",
        "# Import necessary libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Load the Iris dataset (suitable for visualization with 3 classes)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names # Get the class names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train an SVM classifier\n",
        "svm_classifier = SVC(kernel='linear', random_state=42) # Using linear kernel for simplicity\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svm_classifier.predict(X_test)\n",
        "\n",
        "# --- Visualize the Confusion Matrix ---\n",
        "\n",
        "# Calculate the Confusion Matrix\n",
        "# The Confusion Matrix is a 2D array where rows represent actual classes and columns represent predicted classes.\n",
        "# Entry (i, j) is the number of samples with actual class i and predicted class j.\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Print the Confusion Matrix (optional)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Visualize the Confusion Matrix using seaborn and matplotlib\n",
        "plt.figure(figsize=(8, 6)) # Set the figure size\n",
        "\n",
        "# Use seaborn.heatmap to create the heatmap\n",
        "# annot=True: Annotate the heatmap with the data values\n",
        "# fmt='d': Format the annotations as integers\n",
        "# cmap='Blues': Use a colormap\n",
        "# xticklabels, yticklabels: Set the labels for the x and y axes (predicted and actual class names)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.show()\n",
        "\n",
        "# --- Alternatively, use ConfusionMatrixDisplay from sklearn ---\n",
        "# ConfusionMatrixDisplay is newer and can be more convenient\n",
        "\n",
        "print(\"\\n--- Using ConfusionMatrixDisplay ---\")\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "cmd.plot(ax=ax, cmap='Blues', values_format='d')\n",
        "\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for SVM Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ttF-vdf-i6wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 24.  Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n",
        "# Error (MAE) instead of MSE\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_absolute_error # Import mean_absolute_error\n",
        "\n",
        "# Load a housing dataset (using California Housing dataset as an example)\n",
        "# If you have your own dataset, load it here\n",
        "try:\n",
        "    housing = datasets.fetch_california_housing()\n",
        "    X = housing.data\n",
        "    y = housing.target\n",
        "except AttributeError:\n",
        "    print(\"California Housing dataset not available. Using make_regression as an example.\")\n",
        "    from sklearn.datasets import make_regression\n",
        "    X, y = make_regression(n_samples=200, n_features=10, noise=10, random_state=42)\n",
        "\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an SVR regressor\n",
        "# You can experiment with different kernels and parameters\n",
        "svr_regressor = SVR(kernel='linear') # Using linear kernel as an example\n",
        "\n",
        "# Train the regressor\n",
        "svr_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = svr_regressor.predict(X_test)\n",
        "\n",
        "# --- Evaluate the regressor using Mean Absolute Error (MAE) ---\n",
        "\n",
        "# Calculate Mean Absolute Error\n",
        "# MAE is the average of the absolute differences between the actual and predicted values.\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "# Print the Mean Absolute Error\n",
        "print(f\"Mean Absolute Error (MAE) of the SVR regressor: {mae:.2f}\")\n",
        "\n",
        "# --- You can also calculate MSE for comparison if you want ---\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# mse = mean_squared_error(y_test, y_pred)\n",
        "# print(f\"Mean Squared Error (MSE) of the SVR regressor: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "OLeCNkkCi6us"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 25 . Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n",
        "# score\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score # Import ROC/AUC functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset (a binary classification dataset)\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target # 0 for malignant, 1 for benign\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Gaussian Naive Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# --- Evaluate using ROC-AUC Score ---\n",
        "\n",
        "# Get the probability estimates for the positive class (class 1)\n",
        "# roc_curve and roc_auc_score require the probability of the positive class\n",
        "# The output of predict_proba is a 2D array: [[prob_class_0, prob_class_1], ...]\n",
        "y_pred_proba = gnb.predict_proba(X_test)[:, 1] # Select probabilities of the positive class (index 1)\n",
        "\n",
        "# Calculate the ROC curve points\n",
        "# fpr: False Positive Rate\n",
        "# tpr: True Positive Rate (Recall)\n",
        "# thresholds: Thresholds used to compute fpr and tpr\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
        "\n",
        "# Calculate the Area Under the ROC Curve (AUC)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Alternatively, use roc_auc_score for a single score calculation\n",
        "roc_auc_single = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "\n",
        "# Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc_single:.2f}\")\n",
        "\n",
        "# --- Visualize the ROC curve (optional) ---\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random (AUC = 0.50)') # Baseline random classifier\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
        "plt.ylabel('True Positive Rate (Recall)')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EsInbf1ui6sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score # Import PR curve functions\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset (a binary classification dataset)\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target # 0 for malignant, 1 for benign\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train an SVM classifier\n",
        "# Note: For precision_recall_curve, the classifier needs to output probability estimates or decision function values.\n",
        "# SVC with probability=True can output probabilities, but decision_function is often preferred for PR curves.\n",
        "svm_classifier = SVC(kernel='linear', random_state=42, probability=True) # Set probability=True to get probabilities\n",
        "# Or use decision_function:\n",
        "# svm_classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# --- Visualize the Precision-Recall Curve ---\n",
        "\n",
        "# Get the probability estimates of the positive class (class 1)\n",
        "# precision_recall_curve and average_precision_score can use probabilities or decision function values.\n",
        "# Using probabilities (requires probability=True in SVC):\n",
        "y_scores = svm_classifier.predict_proba(X_test)[:, 1]\n",
        "# Or using decision_function (often preferred for PR curves with SVM):\n",
        "# y_scores = svm_classifier.decision_function(X_test)\n",
        "\n",
        "\n",
        "# Calculate the Precision-Recall curve points\n",
        "# precision, recall, thresholds = precision_recall_curve(y_test, y_scores) [1]\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores) # _ is for thresholds\n",
        "\n",
        "# Calculate the Average Precision (AP) score\n",
        "# AP is the area under the Precision-Recall curve.\n",
        "average_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Print the Average Precision score\n",
        "print(f\"Average Precision (AP) Score: {average_precision:.2f}\")\n",
        "\n",
        "# --- Visualize the Precision-Recall curve ---\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='darkorange', lw=2, label=f'Precision-Recall curve (AP = {average_precision:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O2DSyBqYi6qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ujFKrI99i6oG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}